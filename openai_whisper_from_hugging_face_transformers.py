# -*- coding: utf-8 -*-
"""OpenAI Whisper from Hugging Face Transformers

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z3uNwqxl3r3UpMhCUwAMo3zXZtOETaT2
"""

!nvidia-smi

! pip install git+https://github.com/huggingface/transformers -q

#!wget -O audio.mp3 http://www.moviesoundclips.net/movies1/darkknight/criminal.mp3
!pip install pytube3
!pip install pydub

from IPython.display import Audio, display
from pydub import AudioSegment
from transformers import pipeline
import tempfile
from pytube import YouTube
from IPython.display import Audio
import tempfile
import os
url = "https://www.youtube.com/watch?v=pS_POUVFXvk"
# Get the YouTube video and audio stream
yt = YouTube(url)
audio_stream = yt.streams.filter(only_audio=True).first()

# Create a temporary file to hold the audio data
with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as temp_file:
    audio_stream.download(filename=temp_file.name)

    # Play the audio
    display(Audio(filename=temp_file.name, autoplay=True))

# Step 1: Split the audio into chunks
audio = AudioSegment.from_file(temp_file.name)
chunks = [chunk for chunk in audio[::30 * 1000]]  # Split audio into 30-second chunks

# Step 2: Transcribe each chunk
whisper = pipeline('automatic-speech-recognition', model='openai/whisper-medium', device=0)


transcriptions = []
for chunk in chunks:
    # Save each chunk to a temporary file
    with tempfile.NamedTemporaryFile(delete=True, suffix=".mp3") as temp_file:
        chunk.export(temp_file.name, format="mp3")
        transcription = whisper(temp_file.name)  # Pass the file path instead of raw data
        transcriptions.append(transcription)


# Step 3: Combine the transcriptions
full_script = ' '.join(t["text"] for t in transcriptions)

print(full_script)

# Save full_script to a temporary file
with tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='.txt') as temp_file:
    temp_file.write(full_script)
    temp_file_path = temp_file.name  # Save the path to the temp file for later use

print(f'Temporary file created at: {temp_file_path}')  # Output the path to the temporary file

print(full_script)

import os

#from dotenv import load_dotenv
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Pinecone
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA
#import pinecone


documents = TextLoader(temp_file_path).load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=20,
    length_function=len
)

texts = text_splitter.split_documents(documents)



embeddings = OpenAIEmbeddings(openai_api_key=OPEN-AI-KEY)

vector_store = Pinecone.from_documents(
    texts, embeddings, index_name="chat-with-text-file"
)

qa = RetrievalQA.from_chain_type(
    llm=OpenAI(), chain_type="stuff", retriever=vector_store.as_retriever(), return_source_documents=True
)

question = "What is a docker DB? Treat me as a beginner and Give me a short answer"
result = qa({"query": question})

print(result)

print(type(texts))
print(type(embeddings))
print(type("chat-with-text-file"))

# Your code snippet
from langchain.vectorstores import Chroma
loader = TextLoader(temp_file_path, encoding='utf8')
documents = loader.load()

texts = text_splitter.split_documents(documents)
embeddings = OpenAIEmbeddings(openai_api_key="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")
db = Chroma.from_documents(texts, embeddings)

retriever = db.as_retriever()
qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)
answer = qa.run(question)

!pip install chromadb

!pip install tiktoken

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
summarizer(full_script)

# Create a summarization pipeline
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Summarize the text
summary = summarizer(full_script, max_length=150, min_length=25, do_sample=False)

# Output the summary
print(summary[0]['summary_text'])

full_script

from transformes import pipeline

from transformers import pipeline

# Assume full_script contains the text extracted from your video
full_script = " Hello and welcome to my channel to this new video on graph embeddings. Now graph databases enjoy wide enthusiasm and adoption of analytics of integrated data potentially from various sources. However, most machine learning algorithms they work on vector form data and therefore to unlock this powerful toolbox we need a way to map our graph structure to a vector form. And that's what I'm very excited about because I studied the Notarack algorithm and I'm  In this video we will walk through all the important details. We are going to start with the basic motivation why should we care about mapping graph structures to vectors. Secondly we are talking about the problem. So how do we define this problem? What's the context of a node and what's the overall objective? And finally we will walk through the data.  math and here lies the key to the beauty and simplicity of the approach because once we understand the last bit of the equation the novelty of the approach becomes obvious to us. So stay curious it's going to be a pleasure and so rewarding to understand this. Get a coffee, lean back and let's do it. Alright part one the motivation. Why should we care about graph embeddings? So we are trying to find a vector representation for our graph structure and we are not talking about the directly associated features  We want to map the structure of the data, meaning the relationships between the data points to an embedding space so that we can feed this into a machine learning algorithm. There is a strong relationship to the Word2Vec algorithm, which is a word embedding algorithm. So it maps words to an embedding space which share a similar context. However, if we look at a sentence, we can tell that it's a one-dimensional  where each word has a fixed position and two direct neighbors, meaning the word after it and the word before it. However, if we look at a graph, it's represented as an adjacency matrix. So for each node in the graph, we get an entire vector of direct neighbors. So even though the approaches are similar, as the name suggests, we need a slightly different approach for mapping nodes to vectors. Let's go ahead and formulate the general objective of the graph.  We want to map nodes with a similar context, meaning they connect to the same nodes close in an embedding space where they are represented as vectors. Now because you are following so attentive, two questions popped up in your head. First, what does similar context mean? And second, what does close in the embedding space mean? So you are going to be relieved as I tell you that we are going to be talking about exactly about these two questions. Let's have a look at the context of a node.  They access two orthogonal approaches how to explore the context of a node in a graph and I'm pretty sure you're aware of both of them because they are tightly linked to breadth-first and depth-first search. The first similarity matrix is homophily. It's derived from a sociological concept stating that similar people tend to interact close with each other, meaning that similar people have direct relationships and you can already tell why it's related to a breadth-first algorithm.  because this algorithm first explores the direct context of a node before it goes more into depth. The second metric is called structural equivalence and it tries to detect the role of a node from a wider perspective. So a role of a node in a network would be, for example, like in human networks, would be, for example, an influencer or someone who connects communities or something like that. So we're not only looking at the direct relationships of a node, but rather we follow up  on multiple relationships, potentially multiple hops to go into depth to see which other nodes our source node is connected to. This in the graph context is closely related to a depth-first algorithm because there we also follow one relationship of our source node and then we follow up on this relationship to see, okay, whom else can we see if we follow this one relationship. And only if we have explored that one path into depth we will move on to the next direct relationship.  Now I'm pretty sure that you are familiar with depth first and breadth first search in a graph, but I will walk you through quickly here on this slide. Alright, let's start with the depth first algorithm, which is the orange annotation here on this slide. So we start in source node 1 and we would explore all its relationships into depth first before we move on to the next direct relationship. So for example, we start with the relationship 1 and we end up in node 2 and explore along the path until we reach the next direct relationship.  we cannot find any other node more into the depth. But here we would have to jump back to node 2. So we don't do that, but rather we go back in this path and see that in node 2 we have more relationships to explore. So we follow through on here. And from the 5 we don't jump back to the node 1, but rather we explore all the other relationships of 5. So we would continue with node 6 down here. And once we have explored this entire path here into depth,  We would jump back to node 1 and then explore its other relationships. We wouldn't take the relationship to node 5 because we have already explored node 5, but rather we jump to node 3 and explore this relationship in turn into depth. The second approach is a breadth-first algorithm. Here we again start in node 1 and we explore all the direct relationships first.  We jump to node 2, 5 and 3 in the first 3 steps. Then we see we don't have any more direct relationships. So we explore nodes in the distance level of 2. We jump to node 2 and go to all the direct relationships from there. We would end up in nodes 9 and 8 and 7. Then we jump to 5 and go to node 6 and then to 3 and go to node 4. Here we have the context of node 1.  The context of the first algorithm is the breadth first algorithm.  So 7 in total. So we have 4 over 7 would be our similarity of these two nodes. Okay, now we know what the context of a node means. However, we usually we cannot take into account the entire context of a node because that would eventually lead to capturing the entire graph. Therefore, we have to introduce a sampling strategy, which is basically a strategy to approximate the context of a node. In the NotoVEC algorithm, they introduce a sampling strategy  The context exploring is parametrized, which is a trade-off between the breadth-first and depth-first algorithm.  move one distance level away from the source node, which would be similar to structural equivalence, or whether it should stay close to the source node, which would be related to homophily or a more breadth-first algorithm. Now with applying the sampling strategy at a source node in the graph, we can obtain the neighborhood of U given sampling strategy S, which is the context of a node which we are going to take into account and which  The second question was what does close in the embedding space mean?  In that formulation we want to map nodes with a similar context close in the embedding space. That is the foundation of our optimization problem. In the next step we are going to formulate it from a mathematical perspective and explore how NotoVex solves this optimization problem.  for which we want to find a graph embedding, for example. Now, if we explore the context of source node U in the graph, we can find easily the neighborhood of source node U, given the sampling strategy S. And that is the sampling strategy we talked about previously. So we have a parameter which decides how to explore the surrounding of the node U. And we end up with a set of nodes. These are the orange ones here.  In the context of our neighbor of our source node u now I want to define node v as being an element of the context of our node u Okay, having these things at hand. We want to find a mapping function which we call f Which basically maps a node to a vector of the dimensions so f of u is a vector of the dimensions alright having this function f at hand  which are two vectors, we can easily calculate the similarity between two nodes because we just take the two nodes and generate the vector with a generating function f or the mapping function f and then we take the dot product as discussed previously which we can think of the similarity score in the embedding space. So we can think of the dot product as being the angle between the two vectors. So if node v is in the context of u, we want this similarity to be large.  the angle between the two vectors should be small. Now having the dot product as a summarity score in the embedding space, we want to transform that score into probabilities and that we can do by applying a softmax. Because a softmax basically normalizes our score into a probability because it squashes the numbers into an interval of 0 to 1 and all the numbers for node u, so all the combinations of  V and U that we can potentially find in the graph have a sum that equals 1. So that's the softmax. If you're not familiar with the softmax, please research that on your own. Now having the probability, we can actually write this as a conditional probability. So we say the probability that given node U, which was our source node, was the probability for seeing node V, and that we calculate over the function f, which makes a vector out of  nodes and then over the dot product and we apply the softmax and we arrive at that probability which we want to optimize because as we know that node v is in the context of node u we want this to be large so we want to maximize this probability and that's how the relationship is formed between contexts of nodes and similarities in the embedding space. However having that at hand we don't want to do that only for one node  being the context of U but rather for the entire context of U. So we start from a source node U and we want to find the probability for all the nodes in the context of U. So the probability for having this entire context given node U is in turn a conditional probability if we apply it to the entire context of U. And if we assumed independence of the nodes in the sample of  the context of u, we can write this pretty easily as simply a product over all nodes in the context of u. And we take the conditional probability which we saw earlier. So we take probability given node u, seeing one of the nodes in the context of u, and that's an entire product over all of these samples. And where we arrive is basically this up here. So we get the probability for  We are seeing all of the nodes in the context of u of the sample context given that we start in node u. Now this formula is still very easy and if we go ahead and apply this now to all nodes in the graph simultaneously so we not only start from one node u but rather we start from all nodes in the graph a fixed number of random walks so we sample each context of each node in the graph  Then we apply this in our equation. We would add up at the following equation, which is basically the final equation for finding graph embeddings. So here we have the probability for seeing the sample context of node u, given we start in one node u, which is basically this term up here. And then we sum these probabilities up over all nodes  in our graph so every node becomes the node u and that's the entire score for our embedding we found and we of course we want to maximize this equation so we take this equation which is the total probability score which we want to maximize so we want this to be maximized which would be our best graph embedding we could find because nodes which would appear in the context of a node  would have a close or a high probability in the embedding space and that's what we want to find. So we update the f function up here which we use to generate a vector out of a node and maximize for this probability using stochastic gradient descent. And that's basically it. It's that simple. I really like how straightforwardly it is formulated and how  easily it is understandable and not to say that it's easily calculated because there are some challenges in calculating these terms. So there are some optimizations and approximations used to calculate this equation here efficiently on large data sets but you can look them up in the paper as well. But the basic idea is actually that simple. And that's the amazing thing because that's all there is. It's very easily and intuitive formulated  It's a very concise description of the problem we are trying to solve and it's very simple math and still it works so well for many use cases. So if you like this video hit the like button. If you have questions leave them in the comments and make sure you subscribe for the next video where I'm going to be talking about. I don't know yet but we will see. See you then. Bye bye."

# Create a summarization pipeline
summarizer = pipeline("summarization")

# Summarize the text
summary = summarizer(full_script, max_length=150, min_length=25, do_sample=False)

# Output the summary
print(summary[0]['summary_text'])

from transformers import pipeline

def chunk_text(text, max_length):
    chunks = []
    words = text.split()
    current_chunk = ""
    for word in words:
        if len(current_chunk + word) < max_length:
            current_chunk += " " + word
        else:
            chunks.append(current_chunk)
            current_chunk = word
    chunks.append(current_chunk)
    return chunks

# Assume full_script contains the text extracted from your video
full_script =  " Hello and welcome to my channel to this new video on graph embeddings. Now graph databases enjoy wide enthusiasm and adoption of analytics of integrated data potentially from various sources. However, most machine learning algorithms they work on vector form data and therefore to unlock this powerful toolbox we need a way to map our graph structure to a vector form. And that's what I'm very excited about because I studied the Notarack algorithm and I'm  In this video we will walk through all the important details. We are going to start with the basic motivation why should we care about mapping graph structures to vectors. Secondly we are talking about the problem. So how do we define this problem? What's the context of a node and what's the overall objective? And finally we will walk through the data.  math and here lies the key to the beauty and simplicity of the approach because once we understand the last bit of the equation the novelty of the approach becomes obvious to us. So stay curious it's going to be a pleasure and so rewarding to understand this. Get a coffee, lean back and let's do it. Alright part one the motivation. Why should we care about graph embeddings? So we are trying to find a vector representation for our graph structure and we are not talking about the directly associated features  We want to map the structure of the data, meaning the relationships between the data points to an embedding space so that we can feed this into a machine learning algorithm. There is a strong relationship to the Word2Vec algorithm, which is a word embedding algorithm. So it maps words to an embedding space which share a similar context. However, if we look at a sentence, we can tell that it's a one-dimensional  where each word has a fixed position and two direct neighbors, meaning the word after it and the word before it. However, if we look at a graph, it's represented as an adjacency matrix. So for each node in the graph, we get an entire vector of direct neighbors. So even though the approaches are similar, as the name suggests, we need a slightly different approach for mapping nodes to vectors. Let's go ahead and formulate the general objective of the graph.  We want to map nodes with a similar context, meaning they connect to the same nodes close in an embedding space where they are represented as vectors. Now because you are following so attentive, two questions popped up in your head. First, what does similar context mean? And second, what does close in the embedding space mean? So you are going to be relieved as I tell you that we are going to be talking about exactly about these two questions. Let's have a look at the context of a node.  They access two orthogonal approaches how to explore the context of a node in a graph and I'm pretty sure you're aware of both of them because they are tightly linked to breadth-first and depth-first search. The first similarity matrix is homophily. It's derived from a sociological concept stating that similar people tend to interact close with each other, meaning that similar people have direct relationships and you can already tell why it's related to a breadth-first algorithm.  because this algorithm first explores the direct context of a node before it goes more into depth. The second metric is called structural equivalence and it tries to detect the role of a node from a wider perspective. So a role of a node in a network would be, for example, like in human networks, would be, for example, an influencer or someone who connects communities or something like that. So we're not only looking at the direct relationships of a node, but rather we follow up  on multiple relationships, potentially multiple hops to go into depth to see which other nodes our source node is connected to. This in the graph context is closely related to a depth-first algorithm because there we also follow one relationship of our source node and then we follow up on this relationship to see, okay, whom else can we see if we follow this one relationship. And only if we have explored that one path into depth we will move on to the next direct relationship.  Now I'm pretty sure that you are familiar with depth first and breadth first search in a graph, but I will walk you through quickly here on this slide. Alright, let's start with the depth first algorithm, which is the orange annotation here on this slide. So we start in source node 1 and we would explore all its relationships into depth first before we move on to the next direct relationship. So for example, we start with the relationship 1 and we end up in node 2 and explore along the path until we reach the next direct relationship.  we cannot find any other node more into the depth. But here we would have to jump back to node 2. So we don't do that, but rather we go back in this path and see that in node 2 we have more relationships to explore. So we follow through on here. And from the 5 we don't jump back to the node 1, but rather we explore all the other relationships of 5. So we would continue with node 6 down here. And once we have explored this entire path here into depth,  We would jump back to node 1 and then explore its other relationships. We wouldn't take the relationship to node 5 because we have already explored node 5, but rather we jump to node 3 and explore this relationship in turn into depth. The second approach is a breadth-first algorithm. Here we again start in node 1 and we explore all the direct relationships first.  We jump to node 2, 5 and 3 in the first 3 steps. Then we see we don't have any more direct relationships. So we explore nodes in the distance level of 2. We jump to node 2 and go to all the direct relationships from there. We would end up in nodes 9 and 8 and 7. Then we jump to 5 and go to node 6 and then to 3 and go to node 4. Here we have the context of node 1.  The context of the first algorithm is the breadth first algorithm.  So 7 in total. So we have 4 over 7 would be our similarity of these two nodes. Okay, now we know what the context of a node means. However, we usually we cannot take into account the entire context of a node because that would eventually lead to capturing the entire graph. Therefore, we have to introduce a sampling strategy, which is basically a strategy to approximate the context of a node. In the NotoVEC algorithm, they introduce a sampling strategy  The context exploring is parametrized, which is a trade-off between the breadth-first and depth-first algorithm.  move one distance level away from the source node, which would be similar to structural equivalence, or whether it should stay close to the source node, which would be related to homophily or a more breadth-first algorithm. Now with applying the sampling strategy at a source node in the graph, we can obtain the neighborhood of U given sampling strategy S, which is the context of a node which we are going to take into account and which  The second question was what does close in the embedding space mean?  In that formulation we want to map nodes with a similar context close in the embedding space. That is the foundation of our optimization problem. In the next step we are going to formulate it from a mathematical perspective and explore how NotoVex solves this optimization problem.  for which we want to find a graph embedding, for example. Now, if we explore the context of source node U in the graph, we can find easily the neighborhood of source node U, given the sampling strategy S. And that is the sampling strategy we talked about previously. So we have a parameter which decides how to explore the surrounding of the node U. And we end up with a set of nodes. These are the orange ones here.  In the context of our neighbor of our source node u now I want to define node v as being an element of the context of our node u Okay, having these things at hand. We want to find a mapping function which we call f Which basically maps a node to a vector of the dimensions so f of u is a vector of the dimensions alright having this function f at hand  which are two vectors, we can easily calculate the similarity between two nodes because we just take the two nodes and generate the vector with a generating function f or the mapping function f and then we take the dot product as discussed previously which we can think of the similarity score in the embedding space. So we can think of the dot product as being the angle between the two vectors. So if node v is in the context of u, we want this similarity to be large.  the angle between the two vectors should be small. Now having the dot product as a summarity score in the embedding space, we want to transform that score into probabilities and that we can do by applying a softmax. Because a softmax basically normalizes our score into a probability because it squashes the numbers into an interval of 0 to 1 and all the numbers for node u, so all the combinations of  V and U that we can potentially find in the graph have a sum that equals 1. So that's the softmax. If you're not familiar with the softmax, please research that on your own. Now having the probability, we can actually write this as a conditional probability. So we say the probability that given node U, which was our source node, was the probability for seeing node V, and that we calculate over the function f, which makes a vector out of  nodes and then over the dot product and we apply the softmax and we arrive at that probability which we want to optimize because as we know that node v is in the context of node u we want this to be large so we want to maximize this probability and that's how the relationship is formed between contexts of nodes and similarities in the embedding space. However having that at hand we don't want to do that only for one node  being the context of U but rather for the entire context of U. So we start from a source node U and we want to find the probability for all the nodes in the context of U. So the probability for having this entire context given node U is in turn a conditional probability if we apply it to the entire context of U. And if we assumed independence of the nodes in the sample of  the context of u, we can write this pretty easily as simply a product over all nodes in the context of u. And we take the conditional probability which we saw earlier. So we take probability given node u, seeing one of the nodes in the context of u, and that's an entire product over all of these samples. And where we arrive is basically this up here. So we get the probability for  We are seeing all of the nodes in the context of u of the sample context given that we start in node u. Now this formula is still very easy and if we go ahead and apply this now to all nodes in the graph simultaneously so we not only start from one node u but rather we start from all nodes in the graph a fixed number of random walks so we sample each context of each node in the graph  Then we apply this in our equation. We would add up at the following equation, which is basically the final equation for finding graph embeddings. So here we have the probability for seeing the sample context of node u, given we start in one node u, which is basically this term up here. And then we sum these probabilities up over all nodes  in our graph so every node becomes the node u and that's the entire score for our embedding we found and we of course we want to maximize this equation so we take this equation which is the total probability score which we want to maximize so we want this to be maximized which would be our best graph embedding we could find because nodes which would appear in the context of a node  would have a close or a high probability in the embedding space and that's what we want to find. So we update the f function up here which we use to generate a vector out of a node and maximize for this probability using stochastic gradient descent. And that's basically it. It's that simple. I really like how straightforwardly it is formulated and how  easily it is understandable and not to say that it's easily calculated because there are some challenges in calculating these terms. So there are some optimizations and approximations used to calculate this equation here efficiently on large data sets but you can look them up in the paper as well. But the basic idea is actually that simple. And that's the amazing thing because that's all there is. It's very easily and intuitive formulated  It's a very concise description of the problem we are trying to solve and it's very simple math and still it works so well for many use cases. So if you like this video hit the like button. If you have questions leave them in the comments and make sure you subscribe for the next video where I'm going to be talking about. I don't know yet but we will see. See you then. Bye bye."

# Create a summarization pipeline
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Split the text into chunks
max_length = 1024  # Max token limit for the model
chunks = chunk_text(full_script, max_length)

# Summarize each chunk
summaries = [summarizer(chunk, max_length=100, min_length=25, do_sample=False)[0]['summary_text'] for chunk in chunks]

# Combine the summaries
final_summary = " ".join(summaries)

# Output the final summary
print(final_summary)

!pip freeze >requirements.txt

